---
title: "FLUX.1 + LoRA for styled image generation"
author: "Nan Xiao"
date: "2024-09-30T23:47:00"
slug: flux-lora
categories: []
tags:
  - text to image
  - image generation
  - FLUX
  - Diffusers
  - LoRA
  - adapters
  - Hugging Face
meta_img: "image/flux-lora-tech-line-drawing.png"
description: >
    A simple guide to set up and use FLUX.1 with LoRA adapters for AI image
    generation on a local Windows PC. This post covers downloading necessary
    models and software, model file placement, and configuring ComfyUI to run
    the foundation model and add LoRA nodes to the workflow.
---



<div class="float">
<img src="/image/flux-lora-tech-line-drawing.png" alt="Technical line drawing generated by Flux1.dev + LoRA adapter.  Prompt: techlinedrawing, a drawing of a glass-heavy, modern architecture that Californian venture capital firms like, with fine details and mountains in the back, white background." />
<div class="figcaption">Technical line drawing generated by Flux1.dev + LoRA adapter.<br>
Prompt: techlinedrawing, a drawing of a glass-heavy, modern architecture that
Californian venture capital firms like, with fine details and mountains
in the back, white background.</div>
</div>
<p>Besides playing the Elden Ring: Shadow of the Erdtree DLC and Black Myth: Wukong
on my gaming rig, I‚Äôve also been playing with open weights generative AI models
lately. As always, I‚Äôm more than <a href="https://nanx.me/blog/post/pkglite-llm-prompt/">impressed</a>
by the speed of innovation in this space.</p>
<p>For text generation models, at the moment (September 2024), I still feel the
output quality has a bit of a bottleneck on the type of foundation models
we can use and run with limited VRAM at an acceptable speed.
This may change any day in the future, though, so I‚Äôm turning my
attention to the latest image generation models.
Of course, I need to try Flux.1 from Black Forest Labs.</p>
<p>Theoretically, you only need to write a simple Python script to call
the Diffusers package to run the model. I still chose to use a web
GUI here because it‚Äôs nice to have an automated, no-code solution
with a frontend for workflow construction and submitting prompts.</p>
<p>The results feel decent for the <strong>minimal</strong> number of things I tried.
You likely need to experiment with different random number seeds and different
prompts to get there, so it‚Äôs important to have a good GPU or good patience.</p>
<div id="technical-setup-guide" class="section level2">
<h2>Technical setup guide</h2>
<p>At the time of writing, the setup process to get things running is
not exactly apparent, so I will document it here.
This has been tested on a Windows PC with an RTX 4090 (24GB VRAM).</p>
<div id="download-web-gui-software" class="section level3">
<h3>Download web GUI software</h3>
<p>There are many options, but I will use a simple one (ComfyUI) as an example.
It has a built-in Python with essential dependencies like PyTorch and Diffusers.
It might not be <strong>security best practice</strong> to blindly trust code and binaries
on the internet, but hey‚Ä¶ make sure to do your own audit.
The workflow editor frontend is built with their fork of litegraph.js.</p>
</div>
<div id="download-models" class="section level3">
<h3>Download models</h3>
<ol style="list-style-type: decimal">
<li>Download <code>flux1-dev.safetensors</code> and <code>ae.safetensors</code> from
<a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">black-forest-labs/FLUX.1-dev</a>
on Hugging Face. There is also a commercial-friendly model
<a href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">FLUX.1-schnell</a>.</li>
<li>Download <code>t5xxl_fp8_e4m3fn.safetensors</code> and <code>clip_l.safetensors</code> from
<a href="https://huggingface.co/comfyanonymous/flux_text_encoders/tree/main">comfyanonymous/flux_text_encoders</a>.</li>
<li>(Optional) Download your preferred LoRA model (<code>.safetensors</code>) from
<a href="https://huggingface.co/models?other=base_model:adapter:black-forest-labs/FLUX.1-dev">FLUX.1-dev adapters</a> for particular styles (there are 4,000+ of them).
I used <a href="https://huggingface.co/jeremytai/techlinedrawing">jeremytai/techlinedrawing</a>
for the example image at the start of the post.</li>
</ol>
</div>
<div id="place-models-in-the-correct-directory" class="section level3">
<h3>Place models in the correct directory</h3>
<ol style="list-style-type: decimal">
<li><code>ComfyUI/models/unet/</code>: place <code>flux1-dev.safetensors</code></li>
<li><code>ComfyUI/models/vae/</code>: place <code>ae.safetensors</code></li>
<li><code>ComfyUI/models/clip/</code>: place <code>t5xxl_fp8_e4m3fn.safetensors</code> and <code>clip_l.safetensors</code></li>
<li><code>ComfyUI/models/loras/</code>: place your LoRA model‚Äôs <code>.safetensors</code> file</li>
</ol>
</div>
<div id="run-the-foundation-model" class="section level3">
<h3>Run the foundation model</h3>
<ol style="list-style-type: decimal">
<li>Run <code>run_nvidia_gpu.bat</code></li>
<li>Drag the image on the <a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/">ComfyUI Flux examples</a> page into the ComfyUI page to import the Flux workflow.</li>
<li>Select the correct files in the interface:
<ul>
<li>Load Diffusion Model - unet_name: <code>flux1-dev.safetensors</code></li>
<li>DualCLIPLoader - clip_name1: <code>t5xxl_fp8_e4m3fn.safetensors</code>, clip_name2: <code>clip_l.safetensors</code></li>
<li>Load VAE - vae_name: <code>ae.safetensors</code></li>
</ul></li>
</ol>
<p>Now, the workflow is ready to run. Click ‚ÄúQueue Prompt‚Äù to generate the example
image with the default prompt and seed. It took about 10 seconds on my machine.</p>
</div>
<div id="add-lora-node-to-the-workflow" class="section level3">
<h3>Add LoRA node to the workflow</h3>
<ol style="list-style-type: decimal">
<li>Right-click on canvas, and select Add Node &gt; loaders &gt; Load LoRA.</li>
<li>In the new node, select lora_name as the LoRA model file name (<code>.safetensors</code>).</li>
<li>Insert the ‚ÄúLoad LoRA‚Äù node between the ‚ÄúLoad Diffusion Model‚Äù node and
the ‚ÄúBasicGuider‚Äù node, to do this:
<ul>
<li>Connect the Load diffusion model node‚Äôs <code>MODEL</code> output to the
Load LoRA node‚Äôs <code>model</code> input.</li>
<li>Connect the DualCLIPLoader node‚Äôs <code>CLIP</code> output to the
Load LoRA node‚Äôs <code>clip</code> input.</li>
<li>Connect the Load LoRA node‚Äôs <code>MODEL</code> output to the
BasicGuider node‚Äôs <code>model</code> input.</li>
<li>Connect the Load LoRA node‚Äôs <code>CLIP</code> output to the
CLIP Text Encode (Positive Prompt) node‚Äôs <code>clip</code> input.</li>
</ul></li>
</ol>
<p>Write the prompt with the LoRA adapter‚Äôs ‚Äútrigger words‚Äù. The words are often
listed in the <code>README.md</code> files of the LoRA repos.</p>
<p>Click ‚ÄúQueue Prompt‚Äù to generate images. If you click it many times, they
will be queued and generated sequentially. Besides displaying on the interface
once, all generated images will be stored in <code>ComfyUI/output</code>.</p>
<div class="float">
<img src="images/flux-lora-comfyui.png" alt="Running a Flux1.dev + LoRA workflow in ComfyUI." />
<div class="figcaption">Running a Flux1.dev + LoRA workflow in ComfyUI.</div>
</div>
<p>To exit, press Ctrl + C in the terminal and then press Y.</p>
<p>Now I‚Äôve done it ‚Äî writing my first ‚Äúopen source AI‚Äù blog post! ü§ó</p>
</div>
</div>
