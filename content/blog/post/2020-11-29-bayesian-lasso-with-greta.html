---
title: "Bayesian Lasso with greta"
author: "Nan Xiao"
date: "2020-12-07T20:00:00"
slug: bayesian-lasso-with-greta
categories: []
tags:
  - R
  - greta
  - linear models
  - regularization
  - Bayesian lasso
  - automatic differentiation
meta_img: image/horshebend-madeline-pere.jpg
description: "Experimenting with greta for Bayesian sparse regression."
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<blockquote>
<p>The R code to reproduce the results is available from <a href="https://gist.github.com/nanxstats/4b64f81aa258959bef6ca06572307298">GitHub Gist</a>.</p>
</blockquote>
<div class="figure">
<img src="/image/horshebend-madeline-pere.jpg" alt="" />
<p class="caption">Horseshoe Bend, Arizona. Photo by <a href="https://unsplash.com/photos/_QouAO6pJxE">Madeline Pere</a>.</p>
</div>
<p>Although I am not an expert in Bayesian statistics, I always have an idealized version of the framework for Bayesian modeling in my mind:</p>
<ul>
<li>Allows defining data models intuitively — preferably in native R.</li>
<li>Handles the low-level computations such as MCMC automatically.</li>
<li>Works on both CPU and GPU seamlessly would be perfect for 2020.</li>
</ul>
<p>All these features would help me focus on the task instead of the tool better. Among others, <a href="https://mc-stan.org/">Stan</a> and <a href="http://edwardlib.org/">Edward</a> were pretty close to achieving these goals.</p>
<p>Created by Professor Nick Golding, <a href="https://greta-stats.org/">greta</a> is now my all-time favorite. It has all the traits described above. You can see this from its <a href="https://greta-stats.org/articles/example_models.html">example models</a>. To know it better, I experimented with it a bit for fitting Bayesian sparse regression models. I soon realized the simplicity provided by greta could truly enable fast exploration of modeling strategies for a range of problems.</p>
<div id="generate-synthetic-data" class="section level2">
<h2>Generate synthetic data</h2>
<p>Under the linear model <span class="math inline">\(y = X \beta + \varepsilon\)</span>, we will generate 1000 samples: use 500 for fitting the model and leave the rest 500 as the independent test set. The first 10 variables in the total 1000 variables have a non-zero coefficient. A moderate correlation exists between variables. The signal-to-noise ratio (SNR) is also moderate. We simulate the data with <a href="https://nanx.me/msaenet/">msaenet</a>:</p>
<pre class="r"><code>library(&quot;msaenet&quot;)

n &lt;- 500
p &lt;- 1000
pnz &lt;- 10

dat &lt;- msaenet.sim.gaussian(
  n = n * 2, p = p,
  rho = 0.5, coef = rep(5, pnz), snr = 3,
  p.train = 0.5, seed = 42
)

x &lt;- dat$x.tr
y &lt;- dat$y.tr
beta &lt;- c(rep(5, pnz), rep(0, p - pnz))</code></pre>
<div class="alert alert-info" role="alert", style="margin-left:1.5rem;margin-right:1.5rem">

Note: this is for illustrating the modeling processes only. It is not a comprehensive benchmark in any way. Under many other settings and parameter tuning methods, the results can be very different.

</div>
</div>
<div id="multi-step-adaptive-elastic-net" class="section level2">
<h2>Multi-step adaptive elastic-net</h2>
<p>Let’s fit a msaenet model to check if things work, since it offers a look into a pool of models with <span class="math inline">\(\ell_1\)</span> + <span class="math inline">\(\ell_2\)</span> regularizations:</p>
<pre class="r"><code>fit_msaenet &lt;- msaenet(
  x, y,
  family = &quot;gaussian&quot;,
  init = &quot;ridge&quot;, alphas = seq(0.05, 0.95, 0.05),
  tune = &quot;cv&quot;, nfolds = 10, rule = &quot;lambda.min&quot;,
  nsteps = 20, tune.nsteps = &quot;ebic&quot;
)</code></pre>
<div class="figure">
<img src="/image/greta-msaenet-coef.png" alt="" />
<p class="caption">Coefficient path and Extended BIC in 20 adaptive estimation steps.</p>
</div>
<p>We achieved the lowest EBIC in step 2, which is equivalent to an adaptive elastic-net model. We selected 36 variables in total: all the 10 true variables and 26 false positive variables. The MSE is 203.</p>
</div>
<div id="lasso" class="section level2">
<h2>Lasso</h2>
<p>Fit an ordinary lasso model with glmnet:</p>
<pre class="r"><code>library(&quot;glmnet&quot;)

cv_lasso &lt;- cv.glmnet(x, y, family = &quot;gaussian&quot;, alpha = 1, nfolds = 10)
fit_lasso &lt;- glmnet(x, y, family = &quot;gaussian&quot;, alpha = 1, lambda = cv_lasso$lambda.min)</code></pre>
<div class="figure">
<img src="/image/greta-glmnet-cv.png" alt="" />
<p class="caption">The lambda grid and cross-validation errors for lasso.</p>
</div>
<p>The model selected 56 variables in total: all the 10 true variables and 46 false positive variables. The MSE is 202.</p>
</div>
<div id="bayesian-lasso" class="section level2">
<h2>Bayesian Lasso</h2>
<p>Define a Bayesian lasso model using the Laplace prior in greta:</p>
<pre class="r"><code>library(&quot;greta&quot;)

intercept &lt;- normal(0, 10)
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))
coefs &lt;- laplace(0, 1, dim = ncol(x))
mu &lt;- intercept + x %*% coefs
distribution(y) &lt;- normal(mu, sd)

m &lt;- model(intercept, coefs, sd)

draws_blasso &lt;- mcmc(m, warmup = 1000, n_samples = 5000, chains = 8)</code></pre>
<p>The computational graph by plotting <code>m</code>:</p>
<div class="figure">
<img src="/image/greta-bayesian-lasso-graph.png" alt="" />
<p class="caption">Bayesian lasso</p>
</div>
<p>Plot the posterior mean and 95% credible interval of the coefficients:</p>
<div class="figure">
<img src="/image/greta-bayesian-lasso-coef.png" alt="" />
<p class="caption">Posterior of the coefficients from Bayesian lasso. We check whether the 95% credible interval covers 0 to determine if a variable is selected.</p>
</div>
<p>The model selected 16 variables: all the 10 true variables with 6 false positive variables. The MSE is 217.</p>
<p>For more theoretical discussions and empirical comparisons on Bayesian sparse shrinkage in regression, I find the notes from <a href="https://jrnold.github.io/bayesian_notes/shrinkage-and-regularized-regression.html#section">Jeffrey Arnold</a> and <a href="https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html">Michael Betancourt</a> useful. To me, it is still a bit magical how intuitively the lasso and Bayesian lasso are connected. I like this type of connection.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>All three methods correctly selected all the true variables (TP). Regarding the number of false positive variables (FP) and MSE:</p>
<ul>
<li>msaenet: moderate MSE close to Lasso; moderate FP</li>
<li>Lasso: smallest MSE close to msaenet; largest FP</li>
<li>Bayesian lasso: largest MSE (not too bad though); smallest FP</li>
</ul>
<div style="margin-left:1.5rem;margin-right:1.5rem;">
<table class="table table-hover">
<thead>
<tr>
<th style="text-align:center;">
Method
</th>
<th style="text-align:right;">
TP
</th>
<th style="text-align:right;">
FP
</th>
<th style="text-align:right;">
MSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
msaenet
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
203
</td>
</tr>
<tr>
<td style="text-align:center;">
Lasso
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
202
</td>
</tr>
<tr>
<td style="text-align:center;">
Bayesian Lasso
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
217
</td>
</tr>
</tbody>
</table>
</div>
<p>I would not read into this result too much as it only shows a small facet of the possible modeling approaches. It does demonstrate Bayesian Lasso’s potential and the flexibility of greta in probabilistic modeling. I would also try the more recent methods such as <a href="https://github.com/hazimehh/L0Learn">L0Learn</a> and <a href="https://github.com/stephenslab/susieR">susieR</a> in any real tasks as they offer some modern understanding of the problem.</p>
<p>By changing the cross-validation <span class="math inline">\(\lambda\)</span> selection rule from <code>lambda.min</code> to <code>lambda.1se</code> in msaenet and lasso, you will be able to get models with 10 TP variables, 0 FP variables, and an even smaller MSE. It is not used above because I think the rule might introduce an extra “prior” in the Bayesian sense, thus not creating a fair comparison. Similarly, the Bayesian lasso model parameters can also be further tuned, including the priors and variable selection criteria.</p>
<p>I would love to hear your feedback. Please leave a note if you find me made a mistake somewhere or have some suggestions.</p>
</div>
