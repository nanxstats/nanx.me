---
title: "Introducing pytest-r-snapshot: Verifying Python code against R outputs at scale"
author: "Nan Xiao"
date: "2026-01-02T03:32:00"
slug: pytest-r-snapshot
categories: []
tags:
  - Python
  - Python packages
  - pytest
  - snapshot testing
  - test-driven development
  - CICD
meta_img: "image/toa-heftiba-fI9exC1Z2bY-unsplash.jpg"
description: >
  pytest-r-snapshot is a pytest plugin for snapshot testing Python code
  against reference outputs from R. It enables a portable workflow where
  you record snapshots locally with R, then replay them everywhere without R.
---



<div class="float">
<img src="/image/toa-heftiba-fI9exC1Z2bY-unsplash.jpg" alt="An earthy, minimalist display blending textured art with dried and fresh botanicals. Photo by Toa Heftiba." />
<div class="figcaption">An earthy, minimalist display blending textured art with dried and fresh
botanicals.<br />
Photo by <a href="https://unsplash.com/photos/fI9exC1Z2bY">Toa Heftiba</a>.</div>
</div>
<p>First package in the new year: I’m glad to announce <a href="https://nanx.me/pytest-r-snapshot/">pytest-r-snapshot</a>,
a pytest plugin for snapshot testing Python code against reference outputs
generated by R.</p>
<p>You can install it from <a href="https://pypi.org/project/pytest-r-snapshot/">PyPI</a>:</p>
<pre class="bash"><code>pip install pytest-r-snapshot</code></pre>
<p>If your project uses <a href="https://docs.astral.sh/uv/">uv</a>, add it as a development dependency:</p>
<pre class="bash"><code>uv add --dev pytest-r-snapshot</code></pre>
<div id="why-snapshot-testing-against-r" class="section level2">
<h2>Why snapshot testing against R?</h2>
<p>When you’re writing Python code that needs to produce identical results
to existing R code, testing becomes tricky.
You <em>could</em> manually generate expected outputs from R and save them as fixtures
with your tests, but that’s tedious and error-prone.
You <em>could</em> require R and R packages in CI, but that adds infrastructure
complexity and slows things down. What you really want is a way to record
the ground truth once or regularly, commit it, and replay it everywhere.</p>
<p>That’s exactly what pytest-r-snapshot does.</p>
</div>
<div id="workflow-ergonomics" class="section level2">
<h2>Workflow ergonomics</h2>
<p>The pytest plugin is designed for a portable workflow:</p>
<ul>
<li><strong>Record locally</strong> (requires R): run labeled R code chunks embedded in your
Python tests and write snapshot files.</li>
<li><strong>Replay everywhere</strong> (default; no R required): read committed snapshot
files and compare them to Python outputs.</li>
</ul>
<p>The R code chunks live right next to your test assertions. This makes it easy
to see and update what you’re testing against. The snapshots are stored as
plain text files that get committed to version control, so CI runs fast
without needing R.</p>
<p>The best part? The R code blocks follow exactly the same syntax as labeled
R Markdown code chunks, so you can easily copy-paste them from and to
R Markdown or Quarto documents for prototyping and exploration.</p>
</div>
<div id="the-story-behind" class="section level2">
<h2>The story behind</h2>
<p>This plugin grew directly out of our work on
<a href="https://pharmaverse.github.io/rtflite/">rtflite</a>, a Python package for
generating RTF documents for clinical study reports. rtflite aims to
match the output of the R package <a href="https://merck.github.io/r2rtf/">r2rtf</a>,
which meant we needed a reliable way to verify that our Python implementation
produced bitwise identical RTF files after normalization.</p>
<p>We started with an ad hoc approach a year ago: manually running R scripts to
generate reference outputs, saving them to fixture files, and writing custom
code to compare against them. This setup worked, but the mechanism was fragile.
The R code wasn’t version-controlled alongside the tests, updates were manual
and easy to forget, and the whole setup was hard to understand for new contributors.</p>
<p>pytest-r-snapshot formalizes this pattern. You embed the R code directly in
your Python test file (as comments or in multiline docstrings), the plugin
extracts and runs it when recording, and the snapshot files become the source
of truth. When someone reads the test, they can see both the R code that
produced the expected output and the Python code being tested.</p>
<p>Since v2.5.1, the rtflite project has formally
<a href="https://github.com/pharmaverse/rtflite/pull/181">migrated</a> to use
pytest-r-snapshot, which simplified the test infrastructure considerably.</p>
</div>
<div id="how-it-works" class="section level2">
<h2>How it works</h2>
<p>The basic workflow is straightforward. In your test file, embed a labeled
R code chunk as a comment or inside multiline docstrings:</p>
<pre class="python"><code>def test_summary_matches_r(r_snapshot):
    # ```{r, summary}
    # x &lt;- c(1, 2, 3)
    # summary(x)
    # ```

    actual = my_python_summary(...)
    r_snapshot.assert_match_text(actual, name=&quot;summary&quot;)</code></pre>
<p>When you run <code>pytest --r-snapshot=record</code>, the plugin extracts the R code,
runs it, and saves the output to a snapshot file. After that, regular
<code>pytest</code> runs compare your Python output against the recorded snapshot,
without needing R at all.</p>
<p>The plugin supports three modes:</p>
<ul>
<li><code>replay</code> (default): never runs R; fails if the snapshot is missing.</li>
<li><code>record</code>: always runs R and overwrites snapshots.</li>
<li><code>auto</code>: runs R only when a snapshot is missing.</li>
</ul>
<p>You can configure the R execution environment (which <code>Rscript</code> to use,
working directory, environment variables, timeout) either through
command-line options, or pytest configuration in <code>pyproject.toml</code> or
<code>conftest.py</code>.</p>
</div>
<div id="when-to-use-pytest-r-snapshot" class="section level2">
<h2>When to use pytest-r-snapshot</h2>
<p>This pytest plugin is particularly useful when you’re:
porting R packages to Python and need to verify equivalence;
maintaining parallel implementations in both languages;
or building Python tools that need to interoperate with R-based systems.</p>
<p>It’s probably overkill if you just need to test a few simple functions.
But when you’re systematically porting a substantial R codebase, having
automated, version-controlled snapshot tests against the R reference
implementation makes the whole process much more tractable in the long run.</p>
</div>
<div id="learn-more" class="section level2">
<h2>Learn more</h2>
<p>The <a href="https://nanx.me/pytest-r-snapshot/">documentation site</a>
(now built with Zensical!) covers installation, configuration,
and usage patterns in detail.
If you run into issues or have suggestions, please
<a href="https://github.com/nanxstats/pytest-r-snapshot/issues">open an issue</a> on GitHub.</p>
</div>
