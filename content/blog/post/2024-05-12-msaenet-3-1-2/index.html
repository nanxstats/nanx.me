---
title: "msaenet 3.1.2 and a sparse survival modeling example"
author: "Nan Xiao"
date: "2024-05-12T20:10:00"
slug: msaenet-3-1-2
categories: []
tags:
  - R
  - R packages
  - regression
  - sparse linear models
  - variable selection
  - glmnet
  - ncvreg
meta_img: "image/lucas-davies-aG6ByqGXiXg-unsplash.jpg"
description: >
  msaenet 3.1.2 is now available on CRAN. This update introduces a new default
  color palette for plots. In this post, we showcase a high-dimensional
  survival analysis example using msaenet. You will learn how to fit
  parsimonious models and compare model performance by evaluating
  variable selection and prediction metrics.
---



<div class="float">
<img src="/image/lucas-davies-aG6ByqGXiXg-unsplash.jpg" alt="Father-Son Tennis. Photo by Lucas Davies." />
<div class="figcaption"><em>Father-Son Tennis</em>. Photo by <a href="https://unsplash.com/photos/aG6ByqGXiXg">Lucas Davies</a>.</div>
</div>
<p>I’m pleased to announce that <a href="https://nanx.me/msaenet/">msaenet</a> 3.1.2 is now
available on CRAN.</p>
<p>You can install msaenet with:</p>
<pre class="r"><code>install.packages(&quot;msaenet&quot;)</code></pre>
<p>If you frequently build sparse linear models, msaenet can help you generate
more parsimonious solutions with adaptive estimations.
It supports any number of adaptive estimation steps,
flexible initialization methods, multiple model selection criteria,
and automatic parallel parameter tuning.</p>
<div id="new-color-palette" class="section level2">
<h2>New color palette</h2>
<p>This is a more focused update compared to the
<a href="https://nanx.me/blog/post/ggsci-protr-msaenet-release-notes-2024/#msaenet-3.1.1">3.1.1 update in March 2024</a>.
The main user-visible change is the new default color palette (new Tableau 10)
now used for the coefficient profile plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig-color-palettes"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/fig-color-palettes-1.png" alt="Default color palettes: old (top) vs. new (bottom)." width="100%" />
<p class="caption">
Figure 1: Default color palettes: old (top) vs. new (bottom).
</p>
</div>
<p>The new palette has a softer tone, potentially improving accessibility
for users with color vision deficiencies. This color palette is also
applied to all plot types, as seen in the example below.</p>
</div>
<div id="high-dimensional-survival-analysis-example" class="section level2">
<h2>High-dimensional survival analysis example</h2>
<p>Here, we use a simulated dataset with high-dimensional covariates and
survival outcomes to show how msaenet works and compare it with
classic models.</p>
<pre class="r"><code>library(&quot;msaenet&quot;)</code></pre>
<p>First, we create a function to generate simulated data.
The idea is borrowed directly from
<a href="https://glmnet.stanford.edu/reference/Cindex.html">glmnet::Cindex()</a>.
It is straightforward and does not consider factors such as correlation structure
or signal strength/noise level.</p>
<pre class="r"><code>sim_cox &lt;- function(n, p, p_nzv, p_train) {
  x &lt;- matrix(rnorm(n * p), nrow = n, ncol = p)

  nzc &lt;- p * p_nzv
  beta &lt;- rnorm(nzc)
  fx &lt;- x[, seq(nzc)] %*% beta / 3
  hx &lt;- exp(fx)
  ty &lt;- rexp(n, hx)
  tcens &lt;- rbinom(n, prob = 0.3, size = 1)
  y &lt;- survival::Surv(ty, event = 1 - tcens)

  idx_tr &lt;- sample(seq(n), size = round(n * p_train))
  idx_te &lt;- setdiff(seq(n), idx_tr)
  x_tr &lt;- x[idx_tr, , drop = FALSE]
  y_tr &lt;- y[idx_tr, , drop = FALSE]
  x_te &lt;- x[idx_te, , drop = FALSE]
  y_te &lt;- y[idx_te, , drop = FALSE]

  list(&quot;x_train&quot; = x_tr, &quot;y_train&quot; = y_tr, &quot;x_test&quot; = x_te, &quot;y_test&quot; = y_te)
}</code></pre>
<p>Next, we use the function to generate a training set with 1,000 observations
and an independent test set with 1,000 observations. The number of variables
is 2,000, among which only 2.5% (50) are “true” variables with
non-zero coefficients. This makes it a sparse regression problem.</p>
<pre class="r"><code>set.seed(42)
sim_data &lt;- sim_cox(n = 2000, p = 2000, p_nzv = 0.025, p_train = 0.5)</code></pre>
<p>Then we fit an msaenet model with basic setups:</p>
<ul>
<li>Initialization: ridge regression.</li>
<li><span class="math inline">\(\alpha\)</span> tuning grid: 0.05 to 0.95, with step size 0.05.</li>
<li>Parameter tuning for each estimation step: 5-fold cross-validation.</li>
<li>Maximum number of adaptive estimation steps: 10.</li>
<li>Optimal step number selection: Bayesian information criterion.</li>
</ul>
<pre class="r"><code>doParallel::registerDoParallel(parallelly::availableCores())

fit_msaenet &lt;- msaenet(
  sim_data$x_train,
  sim_data$y_train,
  family = &quot;cox&quot;,
  init = &quot;ridge&quot;,
  alphas = seq(0.5, 0.95, 0.05),
  tune = &quot;cv&quot;,
  nfolds = 5L,
  rule = &quot;lambda.1se&quot;,
  nsteps = 10L,
  tune.nsteps = &quot;bic&quot;,
  seed = 42,
  parallel = TRUE,
  verbose = FALSE
)</code></pre>
<p>We generate a coefficient profile plot where the x-axis shows the estimation
step, and y-axis shows how each coefficient changes across all steps.
The optimal step is highlighted with a vertical red dashed line by default.</p>
<pre class="r"><code>plot(fit_msaenet)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig-coef-path"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/fig-coef-path-1.png" alt="Coefficient profile plot showing coefficient changes through adaptive estimation steps." width="65%" />
<p class="caption">
Figure 2: Coefficient profile plot showing coefficient changes through adaptive estimation steps.
</p>
</div>
<p>Switching to <code>type = "criterion"</code> creates a scree plot that shows how the
model selection criterion (BIC here) changes in each step.</p>
<pre class="r"><code>plot(fit_msaenet, type = &quot;criterion&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig-criterion"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/fig-criterion-1.png" alt="Scree plot showing BIC changes through adaptive estimation steps." width="65%" />
<p class="caption">
Figure 3: Scree plot showing BIC changes through adaptive estimation steps.
</p>
</div>
<p>Using <code>type = "dotplot"</code> produces a dot plot showing the “optimal” model
coefficients with various
<a href="https://nanx.me/msaenet/reference/plot.msaenet.html">variable labeling options</a>
available.</p>
<pre class="r"><code>plot(fit_msaenet, type = &quot;dotplot&quot;, label = TRUE)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig-dotplot"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/fig-dotplot-1.png" alt="Dot plot showing the non-zero coefficients from the optimal model." width="65%" />
<p class="caption">
Figure 4: Dot plot showing the non-zero coefficients from the optimal model.
</p>
</div>
<p>We evaluate the number of true positive, false positive, and false negative
selections. We also calculate the concordance index (C-index) on the
training set and the independent test set.</p>
<pre class="r"><code>idx_nzv &lt;- 1L:(2000 * 0.025)

msaenet.tp(fit_msaenet, true.idx = idx_nzv)
#&gt; [1] 29
msaenet.fp(fit_msaenet, true.idx = idx_nzv)
#&gt; [1] 0
msaenet.fn(fit_msaenet, true.idx = idx_nzv)
#&gt; [1] 21

glmnet::Cindex(
  as.vector(predict(fit_msaenet, newx = sim_data$x_train)),
  y = sim_data$y_train
)
#&gt; [1] 0.8431152
glmnet::Cindex(
  as.vector(predict(fit_msaenet, newx = sim_data$x_test)),
  y = sim_data$y_test
)
#&gt; [1] 0.8403838</code></pre>
<p>For comparison, we fit a lasso and a “canonical” elastic-net model
(<span class="math inline">\(\alpha = 0.5\)</span>) with glmnet. An adaptive elastic-net model is available
from the fitted msaenet model.</p>
<pre class="r"><code>doParallel::registerDoParallel(parallelly::availableCores())

cv_lasso &lt;- glmnet::cv.glmnet(
  sim_data$x_train,
  sim_data$y_train,
  nfolds = 5,
  family = &quot;cox&quot;,
  alpha = 1,
  parallel = TRUE
)

fit_lasso &lt;- glmnet::glmnet(
  sim_data$x_train,
  sim_data$y_train,
  family = &quot;cox&quot;,
  alpha = 1,
  lambda = cv_lasso$lambda.1se
)

cv_enet &lt;- glmnet::cv.glmnet(
  sim_data$x_train,
  sim_data$y_train,
  nfolds = 5,
  family = &quot;cox&quot;,
  alpha = 0.5,
  parallel = TRUE
)

fit_enet &lt;- glmnet::glmnet(
  sim_data$x_train,
  sim_data$y_train,
  family = &quot;cox&quot;,
  alpha = 0.5,
  lambda = cv_enet$lambda.1se
)

fit_aenet &lt;- fit_msaenet$model.list[[2]]</code></pre>
<p>We create a few utility functions for computing the same evaluation metrics
used above: TP, FP, FN for variable selection, C-index for predictive performance.</p>
<pre class="r"><code>glmnet.nzv &lt;- function(object) {
  which(abs(as.vector(object$beta)) &gt; .Machine$double.eps)
}

glmnet.tp &lt;- function(object, true.idx) {
  length(intersect(glmnet.nzv(object), true.idx))
}

glmnet.fp &lt;- function(object, true.idx) {
  length(setdiff(glmnet.nzv(object), true.idx))
}

glmnet.fn &lt;- function(object, true.idx) {
  length(setdiff(true.idx, glmnet.nzv(object)))
}

metrics &lt;- function(object, true.idx, data) {
  if (inherits(object, &quot;msaenet&quot;)) {
    tp &lt;- msaenet.tp(object, true.idx = true.idx)
    fp &lt;- msaenet.fp(object, true.idx = true.idx)
    fn &lt;- msaenet.fn(object, true.idx = true.idx)
  }
  if (inherits(object, &quot;glmnet&quot;)) {
    tp &lt;- glmnet.tp(object, true.idx = true.idx)
    fp &lt;- glmnet.fp(object, true.idx = true.idx)
    fn &lt;- glmnet.fn(object, true.idx = true.idx)
  }
  cindex_train &lt;- glmnet::Cindex(
    as.vector(predict(object, newx = data$x_train)),
    y = data$y_train
  )
  cindex_test &lt;- glmnet::Cindex(
    as.vector(predict(object, newx = data$x_test)),
    y = data$y_test
  )
  c(
    &quot;TP&quot; = tp,
    &quot;FP&quot; = fp,
    &quot;FN&quot; = fn,
    &quot;C-index train&quot; = format(round(cindex_train, 4), nsmall = 4),
    &quot;C-index test&quot; = format(round(cindex_test, 4), nsmall = 4)
  )
}</code></pre>
<p>Summarizing all model metrics in a table:</p>
<style type="text/css">
table.table {
  font-family: var(--tw-prose-font-monospace);
  font-size: 1.125rem;
}
</style>
<pre class="r"><code>df &lt;- rbind(
  &quot;lasso&quot;   = metrics(fit_lasso,   idx_nzv, sim_data),
  &quot;enet&quot;    = metrics(fit_enet,    idx_nzv, sim_data),
  &quot;aenet&quot;   = metrics(fit_aenet,   idx_nzv, sim_data),
  &quot;msaenet&quot; = metrics(fit_msaenet, idx_nzv, sim_data)
)

knitr::kable(
  df,
  align = rep(&quot;r&quot;, 5),
  format = &quot;html&quot;,
  table.attr = &quot;class=\&quot;table table-hover\&quot;&quot;
)</code></pre>
<table class="table table-hover">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
TP
</th>
<th style="text-align:right;">
FP
</th>
<th style="text-align:right;">
FN
</th>
<th style="text-align:right;">
C-index train
</th>
<th style="text-align:right;">
C-index test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
lasso
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
89
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.8633
</td>
<td style="text-align:right;">
0.8392
</td>
</tr>
<tr>
<td style="text-align:left;">
enet
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.8538
</td>
<td style="text-align:right;">
0.8341
</td>
</tr>
<tr>
<td style="text-align:left;">
aenet
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
0.8514
</td>
<td style="text-align:right;">
0.8374
</td>
</tr>
<tr>
<td style="text-align:left;">
msaenet
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
0.8431
</td>
<td style="text-align:right;">
0.8404
</td>
</tr>
</tbody>
</table>
<p>We were able to reduce the number of false positive selections from 89
down to 0, with a trade-off of selecting 9 fewer true variables.
For this particular simulated dataset, with fewer variables selected, the
C-index was not affected and even slightly improved on the independent test set.</p>
<p>From the initial experimental results, we see that msaenet likely prioritizes
precision (minimizing false positive selections) over recall
(maximizing true positive selections). While this may not be ideal,
it could be useful, especially in scenarios where the costs of
false positive selections outweigh the costs of false negative selections,
and precision is the priority.</p>
</div>
<div id="how-to-cite-msaenet" class="section level2">
<h2>How to cite msaenet</h2>
<p>If you find msaenet useful, you can cite it as follows:</p>
<blockquote>
<p>Nan Xiao and Qing-Song Xu. (2015). Multi-step adaptive elastic-net:
reducing false positives in high-dimensional variable selection.
<em>Journal of Statistical Computation and Simulation</em> 85(18), 3755–3765.</p>
</blockquote>
<p>A BibTeX entry for LaTeX users is:</p>
<pre class="text"><code>@article{xiao2015multi,
  title   = {Multi-step adaptive elastic-net:
             reducing false positives in high-dimensional variable selection},
  author  = {Nan Xiao and Qing-Song Xu},
  journal = {Journal of Statistical Computation and Simulation},
  volume  = {85},
  number  = {18},
  pages   = {3755--3765},
  year    = {2015},
  doi     = {10.1080/00949655.2015.1016944}
}</code></pre>
</div>
