---
title: "stackgbm is on CRAN"
author: "Nan Xiao"
date: "2024-04-30T8:10:00"
slug: stackgbm-0-1-0
categories: []
tags:
  - R
  - R packages
  - ensemble learning
  - model stacking
  - gradient boosting
  - decision tree
  - GBDT
  - xgboost
  - lightgbm
  - catboost
meta_img: "image/dylan-shaw-AAVR8hB75ko-unsplash.jpg"
description: >
  stackgbm is a minimalist R package for stacking gradient boosting decision
  tree models. It simplifies the process of combining multiple GBDT
  implementations, such as xgboost, lightgbm, and catboost,
  to improve predictive performance on tabular data classification tasks.
  With a focus on simplicity and flexibility, stackgbm could be a useful tool
  for machine learning research and experimentation.
---



<div class="float">
<img src="/image/dylan-shaw-AAVR8hB75ko-unsplash.jpg" alt="A group of sheep on Faroe Islands. Photo by Dylan Shaw." />
<div class="figcaption">A group of sheep on Faroe Islands. Photo by <a href="https://unsplash.com/photos/AAVR8hB75ko">Dylan Shaw</a>.</div>
</div>
<p>I’m happy to announce that stackgbm, a minimalist R package for tree model
stacking, is now on CRAN.
Model stacking is a ensemble learning method that combines the predictions from
multiple base learners to improve overall performance. stackgbm makes it easy
to stack gradient boosting decision tree (GBDT) models, which are particularly
effective for classification tasks.</p>
<p>You can install stackgbm with:</p>
<pre class="r"><code>install.packages(&quot;stackgbm&quot;)</code></pre>
<p>To install all potential dependencies to maximize stackgbm’s capabilities, see the
<a href="https://github.com/nanxstats/stackgbm/wiki/Manage-dependencies">dependency management guide</a>.</p>
<div id="why-tree-models-and-model-stacking" class="section level2">
<h2>Why tree models and model stacking</h2>
<p>Tree-based methods, especially GBDTs, are one of the most successful approaches
for modeling tabular data, similar to the transformer architecture’s impact on
sequence modeling. Naturally, ensemble tree models are a popular topic in
<a href="https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html">machine learning interviews</a>,
where strategies for building tree ensembles are frequently discussed.</p>
<p>Model stacking takes ensemble learning a step further by combining the
strengths of multiple (possibly strong &amp; ensemble) base learners, such as GBDTs.
This approach was effective in some machine learning competitions on
Kaggle and others, making it a worthy strategy to experiment with.</p>
</div>
<div id="stacking-gbdts-with-stackgbm" class="section level2">
<h2>Stacking GBDTs with stackgbm</h2>
<p>stackgbm is a weekend project that implements a classic two-layer stacking model.
The first layer generates numerical “features” (classification probabilities)
using three popular GBDT implementations: xgboost, lightgbm, and catboost.
These features are then fed into a logistic regression model in the second layer
to produce the final classification probabilities.</p>
<p>stackgbm offers convenient wrappers for the GBDT learners. This makes the
entire flow become a few consistent, canonical function calls:</p>
<pre class="r"><code>library(stackgbm)

params_xgb &lt;- cv_xgboost(x, y)
params_lgb &lt;- cv_lightgbm(x, y)
params_cat &lt;- cv_catboost(x, y)

fit &lt;- stackgbm(x, y, params = list(params_xgb, params_lgb, params_cat))

fit |&gt; predict(newx = x_test)</code></pre>
<p>stackgbm is well-suited for for experimentation and research—when you don’t
want to be restricted by machine learning frameworks that require pipeline
construction in a certain way. Its design allows you to easily customize and
integrate into your own workflows, where flexibility and control are critical.</p>
</div>
<div id="simplicity-in-design" class="section level2">
<h2>Simplicity in design</h2>
<p>Model stacking is not a complicated idea, and I believe we can benefit from
keeping the software implementations simple, too.
Inspired by projects like <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>
and <a href="https://github.com/tinygrad/tinygrad">tinygrad</a>, stackgbm focuses on three
core abstractions:</p>
<ul>
<li><strong>Base learner wrappers</strong>: training and inference interfaces.</li>
<li><strong>Hyperparameter tuning</strong>: cross-validation over a parameters grid.</li>
<li><strong>Stacking algorithm</strong>: Fit a two-layer model.</li>
</ul>
<p>By striking a balance between transparency, flexibility, and performance,
I hope stackgbm could provide a useful baseline for tree model stacking,
with sensible defaults and minimal indirection.</p>
<p>For suggestions, please <a href="https://github.com/nanxstats/stackgbm/issues">create an issue</a>
or send a pull request.</p>
</div>
